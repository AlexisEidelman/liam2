 --------------------------------------
 TODO
 --------------------------------------

 - cleanup/sort/update this TODO file

 - move all these TODO to trac tickets



 - kill "kind" keyword in import (make type & fields keyword synonyms)

 - dump and groupby should produce a wrapper for an ndarray with headers and
   show and csv should have the options to display headers or not, totals or
   not and how to display missing values
   internally csv and show should use the same code for that
   >>> what if you want to compute something on one of those ndarray wrapper,
       including the "totals" 

 - move groupby out of alignment.py
 
 - add command to console to list macros

 - make "othertable" work without a PERIOD column
 
 - make "variable checking" (collect_variable) prefix variable by their
   entity and globals by their "table"

 - merge code for loading ndarrays and tables (ie importer.load_ndarray and
   importer.CSV(), they are both special cases of nd record arrays which I
   would like to support at some point. For that I will need: 

   - support loading nd arrays from csv files formatted like "tables" (one
     column per field). The only difference is that the last dimension is
     not "expanded" horizontally. 

   - support nd array of "record" arrays (if using the "table" format, this is
     only a matter of "folding" the non record "dimension"/columns (ie checking
     that all combination of values are present -- because I don't want to
     support sparse arrays yet, that they are correctly sorted -- else I should
     sort them)
     
   - support loading "tables" with the last "dimension" expanded horizontally
     this means the "data" in those expanded columns is not named explicitly
     (at least with the current format)
     see person_salary.csv. This implies that the last column is a "dimension"
     (ie, all rows have values for all the possible values for that field -- 
     it can be the "missing" value though)
   
 - change the format for ndarray/alignement files to be the same as groupby
   output (or the opposite). See person_salary2.csv for an example:
   
   ,,,,,,,period,2001,2002,2003
   co_alive,partner_id,hh_id,f_id,m_id,extra,gender,id,,,
     
   VS (current format):
     
   co_alive,partner_id,hh_id,f_id,m_id,extra,gender,id,period,,
   ,,,,,,,,2001,2002,2003
 
 - somehow prevent running two simulations on the same output file at the same
   time. I have a report that it causes all sorts of strange problems but I
   could not reproduce it here (on my very small test case).
 
 ? if we make fields listing optional in simulation, we could still check
   whether needed fields are present or not (by analysing the simulation
   code), but we couldn't check that the type of fields is "correct". We would
   also need to postpone "compiling" to numexpr (which uses types) to after
   the input data file is loaded (or at least known and analysed).
   I like the idea, because it brings us closer to Python but it would be much
   work for only a small benefit.
 
 - allow to "import" globals during the simulation (effectively allowing to
   bypass the import step for time series and other csv files)
 
 - set config.debug through a command line argument or environment variable
   so that I can have it True without changing config.py 
 
 - expand release script to automate as much as possible:
   * make Bundle files
   * upload to server
   
 - build my own fromiter because np.fromiter is really a pain
   * it fails with the dreaded "iteratator too short" on any exception
     including when a row has too few columns  
   
 - change dtype method on expr to return (shape, dtype) or something like that
   the immediate goal is to be able to distinguish between arrays and scalars
   in advance in methods like Variable.__getitem__
 
 - when trying to "import" a simulation file or trying to "run" an import file,
   output an informative error

 - check versions of dependencies on startup, instead of letting it crash
   randomly in the middle of a simulation with an error message difficult
   to understand for users (for example with numexpr 1.4.2)

 - write a known issues documentation
   * ctx filter
   * 31 vars
   * globals in a lag
   * filter in regressions

 - when there is a divide by zero or 0/0, numpy outputs a RuntimeWarning
   which seems to confuse users (especially 0/0 which produces a cryptic
   warning: RuntimeWarning: invalid value encountered in double_scalars
   This can happen at least in a groupby() call with percent=True when the
   total is 0.0 (for example if expr = grpcount() and there is no individual
   matching the filter - see example in the test simulation)

   The situation in release 0.5.1 was even messier because of the code of
   avglink which disabled the warning in the middle of the simulation and
   did not restore the behaviour afterwards, which means that if we had
   an avglink before an error, we did not get any warning, but we did get some
   if we did not use any avglink or avglink was after the error.

   I think the ideal solution here would be to let the user configure what he
   wants (raise/warn/ignore) but show him the line number in his code, though
   that will be hard to achieve. Easier to achieve would be to print the expr
   where it happened. It should be possible to do this by using np.seterrcall:
   >>> old_handler = np.seterrcall(my_handler)
   >>> old_err = np.seterr(all='call')
   and inspect the caller frame at that point (using sys._getframe([depth])
   
   it would be cleaner to use np.seterr(all='raise') and catch the 
   exception in Expr.evaluate, possibly simply using add_context. But that
   would only work for the "raise" behaviour which is not what all users want,
   since divide by 0 and 0/0 can be normal/expected result.
    
   for the "warn" or "message" case, I think the simplest option is to use
   the caller frame hack. Another option would be to use seterr("raise") at
   first but in the exception handler (at expr level), use seterr("ignore")
   and re-evaluate the expression. Hmmm, that would only work once per class
   of exception (invalid, overflow, ...), not once per exception localtion, 
   as warnings do (and this is the behavior we would like).
   
   yet another option is to call np.seterrcall(my_handler) before any expression
   is evaluated by numpy. This might be slow though. We would use something
   like:
   def make_err_handler(expr):
       msg = str(expr)
       def handler(a, b, c): # check what the signature need to be
           raise/print/... msg
       return handler
   def evaluate(self):
       myhandler = make_err_handler(self) 
       np.seterrcall(myhandler)
       [do real stuff here]

 - make an installer with shortcuts, etc...
   even though cx_freeze can produce basic installers and works fine to produce
   the executable, their installers is not sufficient in our case
   since we need a shortcut to notepad, not to the liam2 simulation engine
   I guess I'll simply use NSIS

 - improve examples:
   * comment each new stuff as it appears the first time
   ? kill logit_regr without expr (logit_regr(0.0, align='al_p_dead_m.csv'):
     use align instead?
   * kill children link, use it or at least add a comment about it
     (it is declared but not used in any example)

 - implement all() and any()

 - link trac to our svn repository (or move the repository on that machine)

 - countlink(persons) -> persons.count()

 - refactor/redesign the clunky parts
   * the goals are:
     - to make things testable by real unittests
     - to have better separations of concepts / clearer "scopes" for each
       class
   * all entities in context
   * hide id_to_rownum
   * .array vs .table vs .lagged_array vs Entity vs EntityContext
     vs full_context vs mapping passed to numexpr

     .array doesn't need to be entirely in memory, .table
     does not need to be on disk, but we need separate concepts in some places
     because they need to support different functionalities: enlarge, remove,
     add column, delete column for .array and only .append for .table

 - find a clean way to make variants, to avoid having 54 different files
   with slight differences in MIDAS.
   * includes might be enough

 - use numexpr.E instead of passing strings

 - rewrite the whole import process: load boolean fields as int8 (shouldn't
   need more work/conversion),
   then do a n-way merge, then do interpolation.
   If we haven't done the bool->int8 migration for the simulation code then
   we will also need to convert boolean fields back to booleans.
   In that case, we will have to copy the data using: b = i8 == 1, otherwise
   "missing" (-1) evaluates as True, which is not what we want in most case
   (and for backward compatibility)

 - fix the "contextual filter" mess: it is taken into account for grpgini &
   grpsum but not grpavg. The problem is that implementing it for grpavg would
   break difficult_match for example.
   I think the best way out of this is to ignore the contextual filter in all
   aggregate functions. But in that case, what about ctx filter in new(),
   matching(), ...?

 - "compile" expressions to another representation which:
   * has an uniform interface, so that it can be walked/traversed
     recursively without having to define a traverse method on most
     expression classes.
   * has not its __eq__ method overloaded so that I can use "expr in d"
     (dict.__contains__(k) calls k.__eq__) so that I can store
     expressions in a dict to check whether they occur more than once

   see ASTNode class in numexpr. Use it as is?

   Questions:
   * use ASTNode class in numexpr as is?
   * would an uniform interface make things cleaner for current
     functionalities (mainly as_string, collect_variables and simplify)?

 - fix globals in a lag

 - if an entity is only used through links (no process is run directly on that
   entity), the data for that entity is not loaded!

 - somehow work around numexpr 2.0 limitation of 31 variables (create hidden
   temporary variables?)

 - better document csv format: -- for missing

 - the filter argument in all regressions without alignment is ignored.
   I should either fix that or remove the argument.

 - launch external command during simulation (for example external model in
   another program). That should be easy. However, to be useful, we need
   to be able to load back the result in the middle of a simulation.

 - test the speed difference of loading & writing each column individually,
   instead of all together
   * both using a table (as is done currently) and storing columns individually
   * if is not too bad, it would enable us to load fields as late as possible
     and write/flush them to disk (removing them from memory) as early as
     possible.
   * it would save some memory, possibly much of it in combination with
     reordering of processes.

 - compressing main "table" (ie the columns) with carray:
   ac[indice_vector] is 1000x slower than a[indice_vector] !
   ac[bool_vector] is 4.5x slower than a[bool_vector]
   ac.sum() is 2.8x slower than np.sum(a) (np.sum(ac) doesn't work)
   bincount(ac) is 100x slower than bincount(a)
   ca.eval(expr, out_flavor='numpy') is 2x slower than ne.evaluate(expr)
   ca.eval(expr) is 6x slower than ne.evaluate(expr)

 - chunking to simulate any size of data:
   x alignment will be hard to do right *when there is ranking involved*
     because if we simply apply alignment to each chunk:
     * the individual paths would potentially be much worse, because in each
       chunk it would take the x% best scoring individuals *but* the best in
       a chunk could very well be the worst of another chunk and thus wouldn't
       be selected if the whole population was simulated at once.
     * the total number would be slightly less precise because we would round
       to integers in each chunk instead of for the whole population. This can
       be mitigated/corrected by storing the error in each chunk and adapt
       the target in the next chunk
     * if there is no deterministic ranking, it should be as good as a single
       block simulation
     * if the ranking score has only a small set of possible values,
       we could partition individuals using their values, eg with integer:
       partition 1: values 0-10
       partition 2: values 11-20
       etc...
       This should allow to treat/sort chunks separately without having to
       reconstruct the complete array in memory (see mapreduce sort).
       However, I don't think this is the case here, so I'm stuck...
   x matching is problematic
   x new is problematic as it breaks the implicit sort by id of the table
     > I have to "block" other processes during the new. The new itself can
     > be done progressively as it doesn't modify any variable
   x remove seem to be ok

 ? data import from within the console?
 - allow input fields to not be in the output
 - document what each regression function does
 ? use more python-like syntax: use = instead of :

 - review field def: do input fields need to be declared at all?
   I could collect variables from all expressions and check that they are
   all present in the input file.
   however, some information about input fields will need to be defined anyway:
   enumerations, ranges, default values, ... (unless I store all that in the
   input file metadata)
 - review the whole filter concept. The missing "else" value is often a problem,
   but can we avoid it? do we want to avoid it?
 - review align() function/alignment process (I don't want to code anything just
   yet, I just want to know if the current syntax will need to be modified).
   * should it return false for all the persons which are not selected? instead
     of the current weird situation: False if stored in a temp variable,
     otherwise don't modify those which are not selected.
   * what would the syntax be for multi-value alignment (when there are more
     than two possible outcomes)
     align([2, 3, 4]

 non feature tasks
 -----------------

 - add explanation of what you can do with the website both on the site and
   in the doc
 - reorder the doc to be more logical (like in my presentations)
 - add unit tests

 missing features to run an automatically translated MIDAS simulation
 --------------------------------------------------------------------

 - implement many2many. Expressing the child (pc) relationship as a (simple)
   one2many is inherently broken!
   * transparently create extra array and table (eg p_p_children) with 3 fields:
     - period
     - p_id
     - c_id
   * store a "pointer" to the array into the Link instance
   * store a "pointer" to the table in ???
   * adapt countlink and sumlink, shouldn't be a problem
   * how do we set them??? via an action:
     add_child: append('pc', child_id)
     remove_child: remove('pc', child_id)

   * express it as a "complex" one2many:
     children: {type: one2many, target: person, field: m_id}
     children: {type: one2many, target: person, fields: (m_id, f_id)}
     children: {type: one2many, target: person, fields: (m_id | f_id)}

     select p2.id, sum(p1.age)
     from person p1 join person p2
     where p1.father_id=p2.id
     group by p2.id

     strictly speaking, this version is more correct but should yield the same
     result:

     where (p2.male and p1.father_id=p2.id) or (not p2.male and p1.mother_id=p2.id)

   * simply work around it
     children_as_mother: {type: one2many, target: person, field: m_id}
     children_as_father: {type: one2many, target: person, field: f_id}
     nch_mother: countlink(children_as_mother)
     nch_father: countlink(children_as_father)
     nch: nch_mother + nch_father

 - missing values for int columns

   using floats everywhere should work (even for "id" and "link" columns (it
   would only limit the number of individuals per entity to 2^24 = ~16 millions
   for float32 or 2^53 = ~9 * 10^15 for float64)

 - missing values for boolean columns (see test_missing.py)

   one option would be to use a boolean mask (and possibly masked arrays in
   numpy) internally/for computations, but use special values in the output
   (because that's much more readable)

   if using floats / NaN:
   ======================

   a & b -> a * b
   a | b -> (a + b) > 0 # Wrong: > 0 loose the NaNs and NaN > 0 is False, not NaN
         -> where(a1 + a2 == 2, 1, a1 + a2)
   ~a    -> 1 - a
   where(a, b, c)
   -> a * b + (1 - a) * c          # Wrong: that leaks NaNs in "unused branch"
   -> where(a == NaN,
            NaN,
            where(a == 1.0, b, c)) # Wrong: can't compare to Nan
   -> where(isnan(a), NaN, where(a == 1.0, b, c))
   -> where(a != a, a, where(a == 1.0, b, c))
   or
   -> where(a == 1.0, b, where(a == 0.0, c, a))
   or
   -> add an new opcode nanwhere_dddd in numexpr:
      nanwhere(a, b, c) => a != a ? a : (a == 1.0 ? b : c)

   if using int / -1:
   ==================

   a & b          -> where((a == -1) | (b == -1), -1, a * b)
   a & b & c      -> where((where((a == -1) | (b == -1), -1, a * b) == -1) | (c == -1),
                           -1,
                           where((a == -1) | (b == -1), -1, a * b) * c)
                  -> where((a == -1) | (b == -1) | (c == -1), -1, a * b * c)
   a | b          -> where((a == -1) | (b == -1), -1, a + b > 0)
   a | b | c      -> where((a == -1) | (b == -1) | (c == -1), -1, a + b + c > 0)
   ~ a            -> where(a == -1, -1, 1 - a)
   where(a, b, c) -> where(a == -1, -1, where(a == 1, b, c))

   bool_expr * float_expr -> where(bool_expr == -1, nan, bool_expr * float_expr)

   where(bool_expr, float_expr1, float_expr2)
      -> where(bool_expr == -1, -1, where(bool_expr, float_expr1, float_expr2))

 - delete "empty" households (dead persons)
   OR
 - implement cascading deletes/death on links
   AND/OR
 - some kind of integrity checks

 bug fixes
 ---------

 - globals should have the correct type, not always "float" (see data_main)
   > is it still the case?
 - nan -> 0 for sum in groupby (see numpy Ticket #1123)

 other missing features
 ----------------------

 ? use stata regressions directly
   * would introduce some kind of dependency on Stata which is not a good thing
     IMO -> convert them first?
 ? importer: import all columns, except a few
 ? make specifying output field types optional (I can compute them):
   listing the fields is technically enough, though having the type make things
   easier and we can have some type checks
 - implement something like "by x, y, z" in stata (though I don't remember what
   it does exactly): it is similar to groupby but can do more stuff.
 - "compress" in stata tries to autodetect the type of each field by
   inspecting the data, and convert it to the smallest type which can handle
   that data.
 - integrate more stuff into the console:
   * import
   * merge
   * drop
 - compression should be user-configurable for the simulation output file
 - allow input fields to not be in the output
   * it might be a good idea to rework the way to declare fields:
     input:
         - earny: float
         - wstatus: int
         - wzas: int
         - yob: int
     output:
         - cscar: int

         - csw: float
         - cswl1: float
         - cswl2: float
         - cswl3: float
         - cswl4: float
         - cswl5: float

         - dunemp: int
         - earny: float
         - secar: int

         - minr60_c: int
         - minr61_c: int
         - minr62_c: int
         - minr63_c: int
         - minr64_c: int
         - minr65_c: int
         - wecar: float
         - workcs: bool
         - workse: bool
         - workwe: bool
         - wstatus: int
         - wzas: int
         - yob: int
         - werb_temp0_sum: float
         - werb_wemin60_sum: float
         - werb_wemin61_sum: float
         - werb_wemin62_sum: float
         - werb_wemin63_sum: float
         - werb_wemin64_sum: float
         - werb_wemin65_sum: float

 ? two different values for missing links:
   * linked to none = -1
   * missing value = -2
   the problem is that in liam1, there wasn't such a difference
 - round(X) should return int
 - warning for unused fields
 ? either allow setting temporary variables in new() (though I'm not sure it's a
   good idea) or produce a better error message when we try to set an
   unknown field
 - better error message when a procedure's processes are missing ordering
   (it is a dict, instead of a list of dict)
   >> we need to to remove the {'predictor': xxx, 'expr': yyy} syntax before
   >> we can do that
 - better error message if unknown entity in simulation.processes (KeyError)
   >> almost done for "init"
 - better error message on invalid (unknown) process name in agespine
 - better error message when an extra comma is used (see test.test_extra_comma)
 - better error message when a temporary variable is used before it is computed
   (the error is not caught by the parser since the variable exists)
   KeyError: '(~to_give_birth)\nto_give_birth'
 - Error messages containing utf8 chars are not displayed on notepad++ or
   dos prompt consoles (py2exe is irrelevant, it just removes the intermediate
   lines of code from the traceback but doesn't change the last lines).
   Example: syntax error if there is a comment containing non-ascii chars inside
   an expression.
 - includes
 - watches in step by step mode (which are recomputed/displayed automatically
   after each step)
 - macros using other macros. This will need:
   1) pre-parse macros to compute which macro depend on which other macro
   2) do a topological sort on them
   3) parse them in order
 - labeled enumerations
   eg workstate: 1: "unemployed, 2: "employee", 3: "civserv", ...
   * display the labels in show/groupby instead of the numeric values
   ? use the labels in "read" expressions (instead of using macros)
     macros contain the == and can contain < or >, which is more generic,
     so I'm not sure it would be an improvement
   * use labels when setting new values (this could be done with macros, but
     that's not how they use it now)
     eg: workstate: "if(RETIREMENTAGE, 10, workstate)"
         workstate: "if(RETIREMENTAGE, RETIRED, workstate)"

            RETIREMENTAGE: "((gender) and (age >= 65)) or ((not gender) and (age >= WEMRA))"
            RETIREMENTAGE: "age >= if(gender, 65, WEMRA)"

 - numpy methods (min/max/...) on zero-length array
 - implement "log" action = show in a file
 - lastpresent(col[, condition]) -> returns the last value which is not missing
   the problem is that if even a single individual never had any value for that
   column, it will go back all the way to the first period
 - initialdata: s/false/value
 - allow setting/creating temporary variables in interactive console
 - merge per_period_fields and globals
   >>> ie make globals writeable
 - line numbers in error messages. It might be tricky because we loose the
   information after yaml parsed the file
 - one filter, several actions.
   eg, when someone dies, you want to change more than one field, and it
   is a bit tedious to need to repeat the filter
   same comment for divorce
 ? move away from yaml for the user...
 - add a concept of "per individual" constant (ie not saved each period)
   (eg errsal)
 - expand variable definition: min, max values, default value, enum, ...
 - new individuals should be able to:
   * use variables' default values
 - explicit function to check if a value is present:
   where(ispresent(err), err, normal(0, 1))
   we can't use != NaN because that wouldn't work for int and bool
   or even floats because NaN != NaN!
 - infer expressions types systematically so that we can do more clever stuff:
   some type checking, correct type for missing values when using filter, and
   missing values handling in general.
 - store some metadata in .h5 (list of entities, start_period, links, ...)
 - implement other historic functions (with missing values):
   * duration limited in time: eg: duration(xxx, 5): max 5 periods
   * durationcurr(variable): duration of the current value of the variable
   * tavg with a filter: only count when it satisfy a filter (eg > 0)
   * currperiodstart(variable): returns the time period the current value
                                of the variable started

 - test/integrate sumatra to launch simulations

 - add "limit" argument to dump()

 - prevent writing too many rows through show(dump())

 optimisation
 ------------

 - try to compress data using google snappy, quicklz or lz4 (they seem even
   faster than fastLZ on which blosc is based)

 - improve numexpr caching algorithm (LRU), and/or increase its size and/or
   cache expressions ourselves. Some long expressions take a lot longer to
   "compile" than to compute on small arrays. Eg minr & friends.

 - store our indices in hdf file (id_to_rownum) so that we wouldn't have to
   compute them for each simulation.
   OR
   use pytables built-in indexing engine (pytables 2.3 and later)

 - optimise memory allocation by building a dependency graph between variables
   and:
   * free temporary variables as soon as they are not needed anymore
   * reorder variables computation (respecting dependencies) to minimize the
     number of temporary variables needed at any one point. See what numexpr
     does to not duplicate effort.
     > that might make logging & debugging harder, so it should be an option

 - optimise "normal" expressions by collecting variables, and replacing them
   with their definitions when they are temporary variables, unless that
   particular variable is used more than N times. (testing should help us
   determine what value to use for N)
   -> this can only be done for variable which do not change between the place
      they are defined and the one they are used
   -> watch out for random functions !
 
 - optimise expressions by factoring out common sub expressions (which are used
   more than N times) in temporary variables

 ? make numexpr able to compute several expressions at the same time
   eg ne.evalutate(["a + (a * b)", "(a * b) * 5", {'a': [1, 2], 'b': [3, 4]})
   >> [3, 10], [15, 40]
   >> this would be limited to expressions which do not "recompute" any of
   >> the variable used
   >> if it is the only factorization, it would limit some of it, because if
   >> some sub expression is common between one expression of the group and
   >> one which is not in the group, there wouldn't be any factorization
   Ideally, I could feed targets too, so that numexpr would be able to do
   the whole thing by itself:
   eg ne.evalutate([('c', 'a + (a * b)'),
                    ('d', '(a * b) * c'),
                    ('a', 'd + 1'),
                    ('c', 'a * 2')], {'a': [1, 2], 'b': [3, 4]})
   >> {'a': ..., 'c': ..., 'd': ...}

 - optimise mean by storing partial sum in an hidden field, and compute
   the mean like this: (sum_last_period + period_value) / num_periods
 - optimise duration by using duration for last period if present
 ? if links are accessed more often than they are modified, it could make sense
   to store row indices in memory (and possibly on disk?), instead of ids.
   It would make both end-of-period data storage (would have to translate back
   to ids) and lifecycle functions (new and remove) slower.
 - do not repartition data for each alignment and each period, but keep the
   groups from period to period and update them when the linked variable
   changes. Or, implement a weaker form of this: cache only within a
   period, so that only the first alignment with a particular combination of
   variables would do the partitioning. eg in MIDAS alignments currently
   (10/12/2012) use only very few different columns:
   * period: 10x
   * age, period: 3x (birth, dead_f, dead_m)
   * agegroup_civilstate, period: 2x (divorce, separate)
   * agegroup_civilstate, civilstate: 2x (mmkt_f, mmkt_m)
   * agegroup_work, period: ~22x !!! (all the others)
 ? translate more performance-critical code to C/Cython
 ? try alternate algorithm for RemoveIndividuals: simply reindex, like I do it
   elsewhere. It might cause a problem if the last ids (newly borns) die.
   it seems to be a tad slower in fact. However, it might be a better starting
   point to translate in C if that is ever necessary.

 projects to keep an eye on
 --------------------------
 
 - Julia (http://julialang.org): a language specifically tailored for
   scientific computing. It seem to have a very interesting feature set
   (optional typing, fast resulting code, builtin parallelisation primitives,
   ...) but a somewhat ugly syntax. What I don't know though is whether its
   "stdlib" is complete enough for our needs (csv input, hdf output, plotting,
   ...).

 - SciDB: http://www.scidb.org   

 - pandas: we could kill half of my code and it would bring for free many
   of the features I want to develop (and more), but this would need such a
   major refactoring that I am uneager to start it :)

 - Numba
 
 - Blaze (http://blaze.pydata.org/) is a generalisation of ndarray with more
   data layouts -- potentially distributed), a delayed-evaluation system
   (like I have done in liam2) and a built-in code generation framework based
   on LLVM (using Numba I think). Documentation is a bit scarce at this point,
   but from my quick glance at it, it seems to be exactly what I have been
   dreaming for but would have been unable to achieve by myself.

 - SymPy. They use mpmath and I wonder how fast it is.

 - bottleneck (http://pypi.python.org/pypi/Bottleneck)
   nansum, nanmin, nanmax, nanmean, nanstd
   it will probably be integrated back into numpy/scipy at some point but it
   will probably take a while, so let's use this in the mean time

 - corepy

 - Theano (http://deeplearning.net/software/theano/)
   it seems to be a PITA to install though (no pre-built packages, needs a
   compiler, BLAS, etc...). It is only integrated with CUDA (ie NVidia)

 - PyOpenCL: OpenCL seem pretty powerful and more standardised than CUDA
   (also works on some CPUs, not only GPUs).

 - scipy.weave.blitz. It seems a bit limited in functionality but it uses
   blitz++ (http://www.oonumerics.org/blitz/) internally which seems to contain
   all I need and more, so adding the missing functionality might not be too
   hard. It seems to be really slow to compile though, so it'll probably be
   only worth it when using very large arrays.

 - ORC (but I would need to create python bindings for it) 
   http://code.entropywave.com/projects/orc/
   or any of the other C JIT referenced in the external links at:
   http://en.wikipedia.org/wiki/Just-in-time_compilation



 GUI
 ---

 - make liam2 useable within Ipython notebook
   * if it doesn't exist yet, develop a web-based generic "table" viewer for
     Ipython (using their archicture). This might need (or not) developing an
     ipython extension
   * on the server side, implement an hdf5 backend
 - assess whether we can get inspiration (or even code) from spyke viewer:
   http://neuralensemble.blogspot.be/2012/11/spyke-viewer-020-released.html
 - try Kivy (kivy.org)

 cleanup
 -------

 - use proper logging & warnings (use logging and warnings modules)
 - move expr to its own directory and split it. Move properties there too. 
 - constants handling is ugly
 - __entity__ vs .entity
 - change constructor arguments to Entity, so that
   1) it is shielded from YAML specifics
   2) I can use the real thing in test_expr instead of FakeEntity
 - factorize/simplify __str__ methods in some way: by using multi-inheritance?
 - merge src/expr with tools/expr
 ? do not store the array/data into the entity. The entity would only be a
   descriptor.

 undecided
 ---------

 ? temporaries which are carried from one period to another
   ex: "progressive" tsum, or "progessive" duration:
     #minr61_c: "duration(minr61_d)"
     minr61_c: "if(minr61_d, minr61_c + 1, minr61_c)"
   we do not (necessarily) need to store the value for each year (unless we
   want to inspect it after-the fact)
    => probleme for the first period
 ? generalize the concept of SubscriptableVariable to "indexed table"
 ? installer
 ? m2o link in aggregate link
   eg, hh_nch: countlink(househould.persons)
   I am not sure it's a good idea after all, because that would introduce
   two ways to do the same thing and the other way (should) already work(s):
       hh_nch: household.get(countlink(persons))
 ? a: "where(b, c[, d])" -> d implicitly = a
 ? specific choice method for boolean: boolchoice(0.3)
   instead of: choice([True, False], [0.3, 1-0.3])
 ? use __filter__ in dump_csv
   in fact the real need is to be able to disable logging easily
   all logging or some of it?

 ? somehow "merge" grpxxx and xxxlink functions (at least for the user, not
   necessarily technically):
   grpcount()            -> count()
   countlink(persons)    -> count(persons)
   >>> this is problematic because count() returns a scalar while
   >>> count(persons) return a vector
   grpsum(age)           -> sum(age)
   sumlink(persons, age) -> [sum(person.age for person in household.persons)
                             for each household]
                             
 ? at the start of each period, wipe out the array (set all to missing)
   => need to use lag explicitly
 ? allow processes to be run only at specific periodicity: eg. some monthly,
   some quarterly, and some annually
 ? store alignment tables in hdf input & output files
   * it seems like a good idea to store them in the output file, so that we can
     know for sure what data was used to generate the output
   * however requiring alignment data to be in the input hdf file, might be
     less convenient for the modeller who might have his source data in csv
     and would thus require an extra import step.
   * on the other hand, having all input data in one file is convenient if they
     want to pass their simulation around
 ? implicit "dead" field. Not sure it is really necessary, as some simulations
   might not involve the creation or death of individuals. And some people
   might want to name it differently: alive, ...
 ? implicit age: I don't like the idea but Geert would like to have it.
 ? implement data generator using variable definitions + custom code where
   it is absolutely needed
 ? explicit "fill missing values" so that you can do operations between vectors
   of a past period.
 ? OR, we could "timestamp" each expression and convert only when using both
   old and current ones in the same expression
 ? automatically add dependencies to simulation? (eg add person.difficult_match
   when person.partner_id is computed)
 ? allow lag to use temporary variables, though it doesn't make much sense.
   it's better to only allow macros
 ? move expression optimisation/simplification to numexpr
 ? "new" action is limited because it can only have one parent, it might be
   interesting to have an arbitrary number of them. eg:
    newbirth: "new('person', filters={'mother': to_give_birth & ~male,
                                      'father': to_give_birth & male},
                   mother_id=mother.id,
                   father_id=father.id,
                   male=logit(0.0),
                   partner_id=-1)"
   however, this might be useless/impractical, as eg, the father filter above
   is wrong, and if an individual is created from several parents, they will
   usually (always?) need to be matched beforehand, so one or several fields
   or links will be available to get to the other parents. Consequently, this
   syntax might be more appropriate:
    newbirth: "new('person', filter={'mother': to_give_birth},
                             links={'father': mother.partner},
                   mother_id=mother.id,
                   father_id=father.id,
                   male=logit(0.0),
                   partner_id=-1)"
   ... but in that case, the current syntax works as well.