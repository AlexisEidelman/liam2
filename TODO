 --------------------------------------
 TODO
 --------------------------------------

 - override input/output (data) files defined in the simulation file through
   command-line arguments
 
 - check that all declared globals are in the dataset
 
 ? check that all used globals are in the dataset
 
 - rewrite the whole import process: load boolean fields as int8 (shouldn't
   need more work/conversion),
   then do a n-way merge, then do interpolation, then convert boolean fields
   back to booleans. We'll have to copy the data using:
   b = i8 == 1, otherwise "missing" (-1) evaluates as True, which is not what
   we want in most case (and for backward compatibility)

 - fix the "contextual filter" mess: it is taken into account for grpgini &
   grpsum but not grpavg. The problem is that implementing it for grpavg would
   break difficult_match for example.
   I think the best way out of this is to ignore the contextual filter in all
   aggregate functions.
 
 - fix globals in a lag
 
 - allow liam1 format (1 column per file) to be imported with the new import
   mechanism

 - somehow work around numexpr 2.0 limitation of 31 variables (create hidden
   temporary variables?)

 - better document csv format: -- for missing
     
 - the filter argument in all regressions without alignment is ignored. 
   I should either fix that or remove the argument.

 - some unit test work
 
 - launch external command during simulation (for example external model in
   another program). That should be easy. However, to be useful, we need
   to be able to load back the result in the middle of a simulation.
 
 - test the speed difference of loading & writing each column individually,
   instead of all together
   * if is not too bad, it would enable us to load fields as late as possible
     and write/flush them to disk (removing them from memory) as early as
     possible.
   * it would save some memory, possibly much of it in combination with
     reordering of processes.
 
 - compressing main "table" (ie the columns) with carray:
   ac[indice_vector] is 1000x slower than a[indice_vector] !
   ac[bool_vector] is 4.5x slower than a[bool_vector]
   ac.sum() is 2.8x slower than np.sum(a) (np.sum(ac) doesn't work)
   bincount(ac) is 100x slower than bincount(a)
   ca.eval(expr, out_flavor='numpy') is 2x slower than ne.evaluate(expr)
   ca.eval(expr) is 6x slower than ne.evaluate(expr)
 
 - chunking to simulate any size of data:
   x alignment will be hard to do right *when there is ranking involved*
     because if we simply apply alignment to each chunk:
     * the individual paths would potentially be much worse, because in each
       chunk it would take the x% best scoring individuals *but* the best in
       a chunk could very well be the worst of another chunk and thus wouldn't
       be selected if the whole population was simulated at once.
     * the total number would be a slightly less precise because we would round
       to integers in each chunk instead of for the whole population. This can
       be mitigated/corrected by storing the error in each chunk and adapt
       the target in the next chunk
     * if there is no deterministic ranking, it should be as good as a single
       block simulation
   x matching is problematic    
   x new is problematic as it breaks the implicit sort by id of the table
     > I have to "block" other processes during the new. The new itself can
     > be done progressively as it doesn't modify any variable
   x remove seem to be ok

 ? data import from within the console?
 - allow input fields to not be in the output
 - document what each regression function does
 ? use more python-like syntax: use = instead of :
 
 - review field def: do input fields need to be declared at all?
   I could collect variables from all expressions and check that they are
   all present in the input file.
   however, some information about input fields will need to be defined anyway: 
   enumerations, ranges, default values, ... (unless I store all that in the
   input file metadata)
 - review the whole filter concept. The missing "else" value is often a problem,
   but can we avoid it? do we want to avoid it?  
 - review align() function/alignment process (I don't want to code anything just 
   yet, I just want to know if the current syntax will need to be modified).
   * should it return false for all the persons which are not selected? instead
     of the current weird situation: False if stored in a temp variable,
     otherwise don't modify those which are not selected.
   * what would the syntax be for multi-value alignment (when there are more
     than two possible outcomes)
     align([2, 3, 4]
 - unit tests !!!!

 non feature tasks
 -----------------

 - add explanation of what you can do with the website both on the site and
   in the doc
 - reorder the doc to be more logical (like in my presentations)
 - add unit tests

 missing features to run an automatically translated MIDAS simulation
 --------------------------------------------------------------------

 - implement many2many. Expressing the child (pc) relationship as a (simple)
   one2many is inherently broken!
   * transparently create extra array and table (eg p_p_children) with 3 fields:
     - period
     - p_id
     - c_id
   * store a "pointer" to the array into the Link instance
   * store a "pointer" to the table in ???
   * adapt countlink and sumlink, shouldn't be a problem
   * how do we set them??? via an action:
     add_child: append('pc', child_id)
     remove_child: remove('pc', child_id)

   * express it as a "complex" one2many:
     children: {type: one2many, target: person, field: m_id}
     children: {type: one2many, target: person, fields: (m_id, f_id)}
     children: {type: one2many, target: person, fields: (m_id | f_id)}

     select p2.id, sum(p1.age)
     from person p1 join person p2
     where p1.father_id=p2.id
     group by p2.id

     strictly speaking, this version is more correct but should yield the same
     result:

     where (p2.male and p1.father_id=p2.id) or (not p2.male and p1.mother_id=p2.id)
     
   * simply work around it
     children_as_mother: {type: one2many, target: person, field: m_id}
     children_as_father: {type: one2many, target: person, field: f_id}
     nch_mother: countlink(children_as_mother)
     nch_father: countlink(children_as_father)
     nch: nch_mother + nch_father

 - missing values for int columns

   using floats everywhere should work (even for "id" and "link" columns (it
   would only limit the number of individuals per entity to 2^24 = ~16 millions 
   for float32 or 2^53 = ~9 * 10^15 for float64) 

 - missing values for boolean columns (see test_missing.py)
 
   if using floats / NaN:
   ======================
   
   a & b -> a * b
   a | b -> (a + b) > 0 # Wrong: > 0 loose the NaNs and NaN > 0 is False, not NaN
         -> where(a1 + a2 == 2, 1, a1 + a2)
   ~a    -> 1 - a
   where(a, b, c) -> a * b + (1 - a) * c -> Wrong: that leaks NaNs in "unused branches"
                  -> where(a == NaN, NaN, where(a == 1.0, b, c)) -> Wrong: can't compare to Nan
                  -> where(isnan(a), NaN, where(a == 1.0, b, c))
                  -> where(a != a, a, where(a == 1.0, b, c))
                  or
                  -> where(a == 1.0, b, where(a == 0.0, c, a))
                  or 
                  -> add an new opcode nanwhere_dddd in numexpr
                     np.where(a != a, float('nan'), np.where(a, b, c))
                     a != a ? a : (a == 1.0 ? b : c)

   if using int / -1:
   ==================
   
   a & b          -> where((a == -1) | (b == -1), -1, a * b)
   a & b & c      -> where((where((a == -1) | (b == -1), -1, a * b) == -1) | (c == -1),
                           -1,
                           where((a == -1) | (b == -1), -1, a * b) * c)
                  -> where((a == -1) | (b == -1) | (c == -1), -1, a * b * c)
   a | b          -> where((a == -1) | (b == -1), -1, a + b > 0)
   a | b | c      -> where((a == -1) | (b == -1) | (c == -1), -1, a + b + c > 0)
   ~ a            -> where(a == -1, -1, 1 - a)
   where(a, b, c) -> where(a == -1, -1, where(a == 1, b, c))
   
   bool_expr * float_expr -> where(bool_expr == -1, nan, bool_expr * float_expr)

   where(bool_expr, float_expr1, float_expr2)
      -> where(bool_expr == -1, -1, where(bool_expr, float_expr1, float_expr2))
                   
 - delete "empty" households (dead persons)
   OR
 - implement cascading deletes/death on links
   AND/OR
 - some kind of integrity checks

 bug fixes
 ---------
 
 - globals should have the correct type, not always "float" (see data_main)
   > is it still the case?
 - nan -> 0 for sum in groupby (see numpy Ticket #1123)

 other missing features
 ----------------------

 ? use stata regressions directly
   * would introduce some kind of dependency on Stata which is not a good thing
     IMO -> convert them first? 
 - data_main: import all columns, except a few
 ? make specifying output field types optional (I can compute them): 
   listing the fields is technically enough, though having the type make things
   easier and we can have some type checks
 - implement something like "by x, y, z" in stata (though I don't remember what
   it does exactly): it is similar to groupby but can do more stuff.
 - "compress" in stata tries to autodetect the type of each field by 
   inspecting the data, and convert it to the smallest type which can handle
   that data.
 - integrate more stuff into the console:
   * import
   * merge
   * drop
 - compression should be user-configurable for the simulation output file
 - allow input fields to not be in the output
   * it might be a good idea to rework the way to declare fields:
     input:
         - earny: float
         - wstatus: int
         - wzas: int
         - yob: int
     output:
         - cscar: int
 
         - csw: float
         - cswl1: float
         - cswl2: float
         - cswl3: float
         - cswl4: float
         - cswl5: float
 
         - dunemp: int
         - earny: float
         - secar: int
 
         - minr60_c: int
         - minr61_c: int
         - minr62_c: int
         - minr63_c: int
         - minr64_c: int
         - minr65_c: int
         - wecar: float
         - workcs: bool
         - workse: bool
         - workwe: bool
         - wstatus: int
         - wzas: int
         - yob: int
         - werb_temp0_sum: float
         - werb_wemin60_sum: float
         - werb_wemin61_sum: float
         - werb_wemin62_sum: float
         - werb_wemin63_sum: float
         - werb_wemin64_sum: float
         - werb_wemin65_sum: float
 
 ? two different values for missing links: 
   * linked to none = -1
   * missing value = -2
   the problem is that in liam1, there wasn't such a difference 
 - data_main:
   * remove the need for objtype_...
   * macro.av: YEAR is not always the 1st row
   * link names
 - round(X) should return int
 - warning for unused fields
 ? either allow setting temporary variables in new() (though I'm not sure it's a 
   good idea) or produce a better error message when we try to set an
   unknown field
 - better error message when a procedure's processes are missing ordering 
   (it is a dict, instead of a list of dict)
   >> we need to to remove the {'predictor': xxx, 'expr': yyy} syntax before
   >> we can do that
 - better error message if unknown entity in simulation.processes (KeyError)
   >> almost done for "init"
 - better error message when an extra comma is used (see test.test_extra_comma)   
 - better error message when a temporary variable is used before it is computed
   (the error is not caught by the parser since the variable exists)
   KeyError: '(~to_give_birth)\nto_give_birth'
 - Error messages containing utf8 chars are not displayed on notepad++ or 
   dos prompt consoles (py2exe is irrelevant, it just removes the intermediate
   lines of code from the traceback but doesn't change the last lines).
   Example: syntax error if there is a comment containing non-ascii chars inside
   an expression.
 - includes
 - watches in step by step mode (which are recomputed/displayed automatically
   after each step)
 - macros using other macros. This will need:
   1) pre-parse macros to compute which macro depend on which other macro
   2) do a topological sort on them
   3) parse them in order 
 - labeled enumerations
   eg workstate: 1: "unemployed, 2: "employee", 3: "civserv", ...
   * display the labels in show/groupby instead of the numeric values
   ? use the labels in "read" expressions (instead of using macros)
     macros contain the == and can contain < or >, which is more generic, 
     so I'm not sure it would be an improvement
   * use labels when setting new values (this could be done with macros, but 
     that's not how they use it now)
     eg: workstate: "if(RETIREMENTAGE, 10, workstate)"
         workstate: "if(RETIREMENTAGE, RETIRED, workstate)" 
      
            RETIREMENTAGE: "((gender) and (age >= 65)) or ((not gender) and (age >= WEMRA))"
            RETIREMENTAGE: "age >= if(gender, 65, WEMRA)"
     
 - numpy methods (min/max/...) on zero-length array
 - implement "log" action = show in a file
 - lastpresent(col[, condition]) -> returns the last value which is not missing
   the problem is that if even a single individual never had any value for that
   column, it will go back all the way to the first period 
 - initialdata: s/false/value
 - allow setting/creating temporary variables in interactive console
 - merge per_period_fields and globals
 - line numbers in error messages. It might be tricky because we loose the 
   information after yaml parsed the file
 - one filter, several actions.
   eg, when someone dies, you want to change more than one field, and it
   is a bit tedious to need to repeat the filter
   same comment for divorce 
 - move away from yaml for the user...
 - filter in all aggregates, not only in grpavg, grpsum & grpgini
 - add a concept of "per individual" constant (ie not saved each period)
   (eg errsal)
 - expand variable definition: min, max values, default value, enum, ...
 - new individuals should be able to:
   * use variables' default values
 - explicit function to check if a value is present:
   where(ispresent(err), err, normal(0, 1))
   we can't use != NaN because that wouldn't work for int and bool
   or even floats because NaN != NaN! 
 - infer expressions types systematically so that we can do more clever stuff: 
   some type checking, correct type for missing values when using filter, and
   missing values handling in general.
 - store some metadata in .h5 (list of entities, start_period, links, ...)
 - implement other historic functions (with missing values): 
   * duration limited in time: eg: duration(xxx, 5): max 5 periods
   * durationcurr(variable): duration of the current value of the variable
   * tavg with a filter: only count when it satisfy a filter (eg > 0)
   * currperiodstart(variable): returns the time period the current value
                                of the variable started

 optimisation
 ------------

 - try SymPy speed. They use mpmath and I wonder how fast it is.
 
 - use bottleneck (http://pypi.python.org/pypi/Bottleneck)
   nansum, nanmin, nanmax, nanmean, nanstd
   it will probably be integrated back into numpy/scipy at some point but it
   will probably take a while, so let's use this in the mean time
 
 - try raw sql (in C)

 - try using corepy 
 
 - try making python bindings to ORC
   http://code.entropywave.com/projects/orc/
   or any of the other C JIT referenced in the external links at:
   http://en.wikipedia.org/wiki/Just-in-time_compilation
 
 - try using Theano (http://deeplearning.net/software/theano/)
   it seems to be a PITA to install though (no pre-built packages, needs a
   compiler, BLAS, etc...). It is only integrated with CUDA (ie NVidia)
   
 - try PyOpenCL: OpenCL seem pretty powerful and more standardised than CUDA
   (also works on some CPUs, not only GPUs).
 
 - try scipy.weave.blitz. It seems a bit limited in functionality but it uses 
   blitz++ (http://www.oonumerics.org/blitz/) internally which seems to contain
   all I need and more, so adding the missing functionality might not be too
   hard. It seems to be really slow to compile though, so it'll probably be 
   only worth it when using very large arrays.
   
 - improve numexpr caching algorithm (LRU), and/or increase its size and/or 
   cache expressions ourselves. Some long expressions take a lot longer to 
   "compile" than to compute on small arrays. Eg minr & friends.

 - store row indices in hdf file
 
 - optimise memory allocation by building a dependency graph between variables
   and:
   * free temporary variables as soon as they are not needed anymore
   * reorder variables computation (respecting dependencies) to minimize the
     number of temporary variables needed at any one point. See what numexpr
     does to not duplicate effort.
     > that might make logging & debugging harder, so it should be an option

 - optimise "normal" expressions by collecting variables, and replacing them 
   with their definitions when they are temporary variables, unless that 
   particular variable is used more than N times. (testing should help us
   determine what value to use for N)
   -> this can only be done for variable which do not change between the place
      they are defined and the one they are used
   -> watch out for random functions !
 - optimise expressions by factoring out common sub expressions (which are used
   more than N times) in temporary variables
 ? make numexpr able to compute several expressions at the same time
   eg ne.evalutate(["a + (a * b)", "(a * b) * 5", {'a': [1, 2], 'b': [3, 4]})
   >> [3, 10], [15, 40]
   >> this would be limited to expressions which do not "recompute" any of
   >> the variable used
   >> if it is the only factorization, it would limit some of it, because if 
   >> some sub expression is common between one expression of the group and
   >> one which is not in the group, there wouldn't be any factorization
   Ideally, I could feed targets too, so that numexpr would be able to do
   the whole thing by itself:
   eg ne.evalutate([('c', 'a + (a * b)'), 
                    ('d', '(a * b) * c'),
                    ('a', 'd + 1'),
                    ('c', 'a * 2')], {'a': [1, 2], 'b': [3, 4]})
   >> {'a': ..., 'c': ..., 'd': ...}
   
 - optimise mean by storing partial sum in an hidden field, and compute
   the mean like this: (sum_last_period + period_value) / num_periods
 - optimise duration by using duration for last period if present
 ? if links are accessed more often than they are modified, it could make sense
   to store row indices in memory, instead of ids. It would make both
   end-of-period data storage (would have to translate back to ids) and
   lifecycle functions (new and remove) slower.  
 ? do not repartition data for each alignment and each period, but keep the
   groups from period to period and update them when the linked variable
   changes.
 ? translate partition_nd to C/cython
 ? translate other performance-critical code to C 
 ? try alternate algorithm for RemoveIndividuals: simply reindex, like I do it
   elsewhere. It might cause a problem if the last ids (newly borns) die.
   it seems to be a tad slower in fact. However, it might be a better starting
   point to translate in C if that is ever necessary. 
     
 cleanup
 -------

 - constants handling is ugly
 - __entity__ vs .entity
 - change constructor arguments to Entity, so that 
   1) it is shielded from YAML specifics
   2) I can use the real thing in test_expr instead of FakeEntity
 - factorize/simplify __str__ methods in some way: by using multi-inheritance? 
 - merge src/expr with tools/expr
 ? do not store the array/data into the entity. The entity would only be a 
   descriptor.

 undecided
 ---------

 ? temporaries which are carried from one period to another
   ex: "progressive" tsum, or "progessive" duration:
     #minr61_c: "duration(minr61_d)"
     minr61_c: "if(minr61_d, minr61_c + 1, minr61_c)"
   we do not (necessarily) need to store the value for each year (unless we
   want to inspect it after-the fact)    
    => problème de la première periode
 ? generalize the concept of SubscriptableVariable to "indexed table"
 ? installer
 ? m2o link in aggregate link
   eg, hh_nch: countlink(househould.persons)
   I am not sure it's a good idea after all, because that would introduce
   two ways to do the same thing and the other way (should) already work(s):
       hh_nch: household.get(countlink(persons))
 ? a: "where(b, c[, d])" -> d implicitly = a
 ? specific choice method for boolean: boolchoice(0.3) 
   instead of: choice([True, False], [0.3, 1-0.3])
 ? use __filter__ in dump_csv
   in fact the real need is to be able to disable logging easily
   all logging or some of it?
 ? explicit file pattern for "csv" function
 ? add "limit" argument to dump()
 ? prevent writing too many rows through show(dump())
 ? somehow "merge" grpxxx and xxxlink functions (at least for the user, not 
   necessarily technically):
   grpcount()            -> count()
   countlink(persons)    -> count(persons)
   >>> this is problematic because count() returns a scalar while
   >>> count(persons) return a vector
   grpsum(age)           -> sum(age)
   sumlink(persons, age) -> [sum(person.age for person in household.persons) 
                             for each household]
 ? at the start of each period, wipe out the array (set all to missing)
   => need to use lag explicitly 
 ? allow processes to be run only at specific periodicity: eg. some monthly,
   some quarterly, and some annually
 ? store alignment tables in hdf input & output files
   * it seems like a good idea to store them in the output file, so that we can
     know for sure what data was used to generate the output
   * however requiring alignment data to be in the input hdf file, might be
     less convenient for the modeller who might have his source data in csv
     and would thus require an extra import step.
   * on the other hand, having all input data in one file is convenient if they
     want to pass their simulation around
 ? implicit "dead" field. Not sure it is really necessary, as some simulations
   might not involve the creation or death of individuals. And some people
   might want to name it differently: alive, ...
 ? implicit age
 ? implement data generator using variable definitions + custom code where
   it is absolutely needed
 ? explicit "fill missing values" so that you can do operations between vectors
   of a past period.
 ? OR, we could "timestamp" each expression and convert only when using both
   old and current ones in the same expression
 ? automatically add dependencies to simulation? (eg add person.difficult_match
   when person.partner_id is computed)
 ? allow lag to use temporary variables, though it doesn't make much sense.
   it's better to only allow macros
 ? move expression optimisation/simplification to numexpr
 ? "new" action is limited because it can only have one parent, it might be
   interesting to have an arbitrary number of them. eg:
    newbirth: "new('person', filters={'mother': to_give_birth & ~male, 
                                      'father': to_give_birth & male},
                   mother_id=mother.id,
                   father_id=father.id,
                   male=logit(0.0),
                   partner_id=-1)"
   however, this might be useless/impractical, as eg, the father filter above 
   is wrong, and if an individual is created from several parents, they will 
   usually (always?) need to be matched beforehand, so one or several fields
   or links will be available to get to the other parents. Consequently, this
   syntax might be more appropriate:
    newbirth: "new('person', filter={'mother': to_give_birth}, 
                             links={'father': mother.partner},
                   mother_id=mother.id,
                   father_id=father.id,
                   male=logit(0.0),
                   partner_id=-1)"
   ... but in that case, the current syntax works as well.